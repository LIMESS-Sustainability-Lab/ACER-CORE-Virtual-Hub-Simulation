{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow\n",
    "!pip install keras\n",
    "!pip install optuna\n",
    "!pip install tqdm\n",
    "!pip install scipy\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install scikit-learn\n",
    "!pip install scikit-image\n",
    "!pip install scikit-learn-intelex\n",
    "!pip install cartopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from data/vypocet adapted.csv...\n",
      "Flattening headers and parsing dates...\n",
      "Converting columns to numeric and cleaning NaNs...\n",
      "Data cleaned: 8388 rows remain.\n",
      "Generating historical metrics...\n",
      "Starting t-Copula simulation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copula u-samples: 100%|██████████| 5000/5000 [00:43<00:00, 113.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverting copula samples back to returns via KDE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invert KDE per asset: 100%|██████████| 3/3 [00:06<00:00,  2.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling copula marginals to match historical volatility...\n",
      "Applying EVT tail-boost...\n",
      "EVT fit: shape=-0.24, scale=738.61\n",
      "Training VAE on return differences...\n",
      "Building Student-T VAE...\n",
      "Epoch 1/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 5.3347 - val_loss: 4.0530 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.4697 - val_loss: 3.4189 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.1964 - val_loss: 3.2742 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.9417 - val_loss: 3.1708 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.7139 - val_loss: 2.9445 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.5782 - val_loss: 2.8351 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.6992 - val_loss: 2.8768 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.5553 - val_loss: 2.8119 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.3097 - val_loss: 2.7762 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.4178 - val_loss: 2.8070 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.3722 - val_loss: 2.6615 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.3242 - val_loss: 2.6149 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.3156 - val_loss: 2.6048 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.2505 - val_loss: 2.5543 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.1395 - val_loss: 2.5301 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.1420 - val_loss: 2.5805 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.2069 - val_loss: 2.4964 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.2231 - val_loss: 2.5571 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.1284 - val_loss: 2.5297 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.1355 - val_loss: 2.4896 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.0714 - val_loss: 2.4581 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.1163 - val_loss: 2.5375 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.0441 - val_loss: 2.5148 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.0808 - val_loss: 2.6957 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.1758 - val_loss: 2.4811 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.0595 - val_loss: 2.3837 - learning_rate: 5.0000e-04\n",
      "Epoch 27/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.0841 - val_loss: 2.4138 - learning_rate: 5.0000e-04\n",
      "Epoch 28/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.0497 - val_loss: 2.3831 - learning_rate: 5.0000e-04\n",
      "Epoch 29/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.0361 - val_loss: 2.4470 - learning_rate: 5.0000e-04\n",
      "Epoch 30/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.0649 - val_loss: 2.3833 - learning_rate: 5.0000e-04\n",
      "Epoch 31/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.0329 - val_loss: 2.4294 - learning_rate: 5.0000e-04\n",
      "Epoch 32/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.0843 - val_loss: 2.4519 - learning_rate: 5.0000e-04\n",
      "Epoch 33/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.0024 - val_loss: 2.4532 - learning_rate: 2.5000e-04\n",
      "Epoch 34/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.9738 - val_loss: 2.3924 - learning_rate: 2.5000e-04\n",
      "Epoch 35/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.9718 - val_loss: 2.4303 - learning_rate: 2.5000e-04\n",
      "Sampling from Student-t prior and decoding in batches...\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m914/914\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "Applying EVT tail-boost...\n",
      "EVT fit: shape=0.11, scale=61.41\n",
      "Final scaling to historical volatility...\n",
      "Computing diagnostics...\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: 'output'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 267\u001b[39m\n\u001b[32m    264\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mResults saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mOUTPUT_CSV\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    266\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 262\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    256\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mComputing diagnostics...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    257\u001b[39m results = pd.DataFrame({\n\u001b[32m    258\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mHistorical\u001b[39m\u001b[33m'\u001b[39m: compute_metrics(hist_losses),\n\u001b[32m    259\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mt-Copula+EVT\u001b[39m\u001b[33m'\u001b[39m: compute_metrics(cop_evt),\n\u001b[32m    260\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mStudentT-VAE+EVT\u001b[39m\u001b[33m'\u001b[39m: compute_metrics(vae_evt)\n\u001b[32m    261\u001b[39m }).T\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m \u001b[43mresults\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mOUTPUT_CSV\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[38;5;28mprint\u001b[39m(results)\n\u001b[32m    264\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mResults saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mOUTPUT_CSV\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marek\\OneDrive\\git-personal\\ACER-CORE-Virtual-Hub-Simulation\\.venv\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marek\\OneDrive\\git-personal\\ACER-CORE-Virtual-Hub-Simulation\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py:3986\u001b[39m, in \u001b[36mNDFrame.to_csv\u001b[39m\u001b[34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[39m\n\u001b[32m   3975\u001b[39m df = \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.to_frame()\n\u001b[32m   3977\u001b[39m formatter = DataFrameFormatter(\n\u001b[32m   3978\u001b[39m     frame=df,\n\u001b[32m   3979\u001b[39m     header=header,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3983\u001b[39m     decimal=decimal,\n\u001b[32m   3984\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m3986\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3987\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3988\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3989\u001b[39m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[43m=\u001b[49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3990\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3991\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3992\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3993\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3994\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3995\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3996\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3997\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3998\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3999\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4000\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4001\u001b[39m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4002\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4003\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marek\\OneDrive\\git-personal\\ACER-CORE-Virtual-Hub-Simulation\\.venv\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1014\u001b[39m, in \u001b[36mDataFrameRenderer.to_csv\u001b[39m\u001b[34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[39m\n\u001b[32m    993\u001b[39m     created_buffer = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    995\u001b[39m csv_formatter = CSVFormatter(\n\u001b[32m    996\u001b[39m     path_or_buf=path_or_buf,\n\u001b[32m    997\u001b[39m     lineterminator=lineterminator,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1012\u001b[39m     formatter=\u001b[38;5;28mself\u001b[39m.fmt,\n\u001b[32m   1013\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m \u001b[43mcsv_formatter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[32m   1017\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marek\\OneDrive\\git-personal\\ACER-CORE-Virtual-Hub-Simulation\\.venv\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:251\u001b[39m, in \u001b[36mCSVFormatter.save\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03mCreate the writer & save.\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    250\u001b[39m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[32m    259\u001b[39m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[32m    260\u001b[39m     \u001b[38;5;28mself\u001b[39m.writer = csvlib.writer(\n\u001b[32m    261\u001b[39m         handles.handle,\n\u001b[32m    262\u001b[39m         lineterminator=\u001b[38;5;28mself\u001b[39m.lineterminator,\n\u001b[32m   (...)\u001b[39m\u001b[32m    267\u001b[39m         quotechar=\u001b[38;5;28mself\u001b[39m.quotechar,\n\u001b[32m    268\u001b[39m     )\n\u001b[32m    270\u001b[39m     \u001b[38;5;28mself\u001b[39m._save()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marek\\OneDrive\\git-personal\\ACER-CORE-Virtual-Hub-Simulation\\.venv\\Lib\\site-packages\\pandas\\io\\common.py:749\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    747\u001b[39m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[32m    748\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[32m--> \u001b[39m\u001b[32m749\u001b[39m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    751\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[32m    752\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m compression != \u001b[33m\"\u001b[39m\u001b[33mzstd\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    753\u001b[39m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marek\\OneDrive\\git-personal\\ACER-CORE-Virtual-Hub-Simulation\\.venv\\Lib\\site-packages\\pandas\\io\\common.py:616\u001b[39m, in \u001b[36mcheck_parent_directory\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m    614\u001b[39m parent = Path(path).parent\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent.is_dir():\n\u001b[32m--> \u001b[39m\u001b[32m616\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33mrf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot save file into a non-existent directory: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mOSError\u001b[39m: Cannot save file into a non-existent directory: 'output'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "from scipy.stats import skew, kurtosis, t as student_t, genpareto, energy_distance\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import warnings\n",
    "\n",
    "# Suppress TensorFlow warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='tensorflow')\n",
    "\n",
    "# -------------------- CONFIGURATION --------------------\n",
    "INPUT_CSV    = os.getenv('INPUT_CSV', 'data/vypocet adapted.csv')\n",
    "OUTPUT_CSV   = os.getenv('OUTPUT_CSV', 'output/ml_benchmark_risk_table.csv')\n",
    "\n",
    "# scenario counts\n",
    "N_SCENARIOS  = 5000\n",
    "\n",
    "# VAE / Copula parameters\n",
    "BATCH_SIZE   = 256\n",
    "EPOCHS       = 100\n",
    "LATENT_DIM   = 12\n",
    "STUDENT_DF   = 3       # heavy-tailed latent prior for VAE\n",
    "EWMA_LAMBDA  = 0.94\n",
    "KDE_BW       = 0.1\n",
    "TAIL_QUANT   = 0.99\n",
    "\n",
    "# PPA constants\n",
    "PPA_VOLUME   = 1.0\n",
    "PPA_STRIKE   = 50.0\n",
    "\n",
    "# -------------------- DATA UTILITIES --------------------\n",
    "def load_and_clean(path):\n",
    "    print(f\"Loading data from {path}...\")\n",
    "    df0 = pd.read_csv(path, sep=';', header=[0,1], engine='python')\n",
    "    print(\"Flattening headers and parsing dates...\")\n",
    "    cols = df0.columns.tolist()\n",
    "    zones, flat = [], []\n",
    "    cur = None\n",
    "    for lvl0, lvl1 in cols:\n",
    "        if not str(lvl0).startswith('Unnamed'):\n",
    "            cur = lvl0.replace('-', '_').lower()\n",
    "        zones.append(None if lvl1.startswith('Unnamed') or lvl1=='Time period' else cur)\n",
    "    for (lvl0, lvl1), z in zip(cols, zones):\n",
    "        name = lvl1.replace(' ','_').replace('(','').replace(')','').replace('-','_').lower()\n",
    "        flat.append(f\"{name}_{z}\" if z else name)\n",
    "    df0.columns = flat\n",
    "    df0['time_period'] = df0['time_period'].str.replace(r'\\s*\\(.*\\)', '', regex=True)\n",
    "    df0['timestamp'] = pd.to_datetime(\n",
    "        df0['time_period'].str.split(' - ').str[0], dayfirst=True, errors='coerce')\n",
    "    df = df0.set_index('timestamp')\n",
    "    # select columns\n",
    "    dec_col = 'decoupling_lost_off_taker_vth'\n",
    "    price_cols = [c for c in flat if 'financial_settlement' in c]\n",
    "    # convert to numeric\n",
    "    print(\"Converting columns to numeric and cleaning NaNs...\")\n",
    "    df[[dec_col] + price_cols] = df[[dec_col] + price_cols].apply(\n",
    "        lambda s: pd.to_numeric(\n",
    "            s.astype(str).str.replace('.', '', regex=False).str.replace(',', '.', regex=False),\n",
    "            errors='coerce'\n",
    "        )\n",
    "    )\n",
    "    df.dropna(subset=[dec_col] + price_cols, inplace=True)\n",
    "    df = df[(df[price_cols] > 0).all(axis=1)]\n",
    "    print(f\"Data cleaned: {len(df)} rows remain.\")\n",
    "    return df, dec_col, price_cols\n",
    "\n",
    "# -------------------- METRICS --------------------\n",
    "def compute_metrics(losses):\n",
    "    return {\n",
    "        'Volatility': float(np.std(losses, ddof=0)),\n",
    "        'Skewness':   float(skew(losses)),\n",
    "        'Kurtosis':   float(kurtosis(losses, fisher=False))\n",
    "    }\n",
    "\n",
    "# -------------------- COPULA SIMULATION --------------------\n",
    "def simulate_t_copula(returns, n_scen, df, ewma_lambda):\n",
    "    print(\"Starting t-Copula simulation...\")\n",
    "    u = returns.rank(axis=0, method='average') / (len(returns) + 1)\n",
    "    z = student_t.ppf(u, df)\n",
    "    weights = ewma_lambda ** np.arange(len(z)-1, -1, -1)\n",
    "    weights /= weights.sum()\n",
    "    cov = np.cov(z.T, aweights=weights)\n",
    "    rng = np.random.default_rng(123)\n",
    "    chi2 = rng.chisquare(df, size=n_scen)\n",
    "    L = np.linalg.cholesky(cov)\n",
    "    sims_u = np.empty((n_scen, len(returns), returns.shape[1]))\n",
    "    for i in tqdm(range(n_scen), desc=\"Copula u-samples\"):\n",
    "        z0 = rng.standard_normal(z.shape)\n",
    "        t_val = (z0 @ L.T) / np.sqrt(chi2[i] / df)\n",
    "        sims_u[i] = student_t.cdf(t_val, df)\n",
    "    return sims_u\n",
    "\n",
    "# -------------------- KDE INVERSION --------------------\n",
    "def invert_kde(u_samples, hist_data, bandwidth):\n",
    "    vals = np.sort(hist_data)\n",
    "    kde = KernelDensity(bandwidth=bandwidth).fit(vals[:, None])\n",
    "    grid = np.linspace(vals.min(), vals.max(), 2000)[:, None]\n",
    "    logp = kde.score_samples(grid)\n",
    "    cdf = np.exp(logp).cumsum()\n",
    "    cdf /= cdf[-1]\n",
    "    inv = np.interp(u_samples.ravel(), cdf, grid.ravel())\n",
    "    return inv.reshape(u_samples.shape)\n",
    "\n",
    "# -------------------- RUN COPULA PIPELINE --------------------\n",
    "def run_copula(df, price_cols):\n",
    "    diffs = df[price_cols].diff().dropna()\n",
    "    sims_u = simulate_t_copula(diffs, N_SCENARIOS, STUDENT_DF, EWMA_LAMBDA)\n",
    "    sims = np.zeros_like(sims_u)\n",
    "    print(\"Inverting copula samples back to returns via KDE...\")\n",
    "    for j in tqdm(range(len(price_cols)), desc=\"Invert KDE per asset\"):\n",
    "        sims[:, :, j] = invert_kde(sims_u[:, :, j], diffs.iloc[:, j].values, KDE_BW)\n",
    "    print(\"Scaling copula marginals to match historical volatility...\")\n",
    "    std_sim = sims.std(axis=(0,1))\n",
    "    std_hist = diffs.std(axis=0).values\n",
    "    sims *= (std_hist / std_sim)\n",
    "    last_prices = df[price_cols].iloc[-1].values\n",
    "    price_paths = last_prices + np.cumsum(sims, axis=1)\n",
    "    idx_dec = price_cols.index('financial_settlement_off_taker_pays_vth')\n",
    "    losses = (PPA_VOLUME * (PPA_STRIKE - price_paths[:, :, idx_dec])).ravel()\n",
    "    del sims_u, sims; gc.collect()\n",
    "    return losses\n",
    "\n",
    "# -------------------- EVT BOOST --------------------\n",
    "def evt_boost(losses, quantile=TAIL_QUANT):\n",
    "    print(\"Applying EVT tail-boost...\")\n",
    "    threshold = np.quantile(losses, quantile)\n",
    "    excess = losses[losses > threshold] - threshold\n",
    "    if excess.size > 0:\n",
    "        shp, loc, scl = genpareto.fit(excess, floc=0)\n",
    "        boosted = threshold + genpareto.rvs(shp, loc=0, scale=scl, size=excess.size)\n",
    "        print(f\"EVT fit: shape={shp:.2f}, scale={scl:.2f}\")\n",
    "        return np.hstack([losses, boosted])\n",
    "    print(\"No extreme exceedances found; skipping EVT.\")\n",
    "    return losses\n",
    "\n",
    "# -------------------- VAE MODEL --------------------\n",
    "class StudentTVAE(Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def call(self, x):\n",
    "        m, lv, z = self.encoder(x)\n",
    "        recon = self.decoder(z)\n",
    "        recon_loss = tf.reduce_mean(tf.square(x - recon))\n",
    "        kl_loss = 0.5 * tf.reduce_sum(tf.exp(lv) + m**2 - 1 - lv, axis=1)\n",
    "        self.add_loss(recon_loss + tf.reduce_mean(kl_loss))\n",
    "        return recon\n",
    "\n",
    "def build_vae(n_assets, prior_df):\n",
    "    print(\"Building Student-T VAE...\")\n",
    "    inp = Input((n_assets,))\n",
    "    x = layers.Dense(128, activation='relu')(inp)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    m = layers.Dense(LATENT_DIM)(x)\n",
    "    lv = layers.Dense(LATENT_DIM)(x)\n",
    "    def sample_latent(args):\n",
    "        mu, logvar = args\n",
    "        eps = tf.random.normal(tf.shape(mu))\n",
    "        return mu + tf.exp(0.5 * logvar) * eps\n",
    "    z = layers.Lambda(sample_latent)([m, lv])\n",
    "    encoder = Model(inp, [m, lv, z], name='encoder')\n",
    "\n",
    "    latent_in = Input((LATENT_DIM,))\n",
    "    y = layers.Dense(64, activation='relu')(latent_in)\n",
    "    y = layers.BatchNormalization()(y)\n",
    "    y = layers.Dense(128, activation='relu')(y)\n",
    "    out = layers.Dense(n_assets)(y)\n",
    "    decoder = Model(latent_in, out, name='decoder')\n",
    "\n",
    "    vae = StudentTVAE(encoder, decoder)\n",
    "    vae.compile(optimizer='adam')\n",
    "    return vae, encoder, decoder\n",
    "\n",
    "# -------------------- RUN VAE PIPELINE --------------------\n",
    "def run_vae(df, price_cols):\n",
    "    diffs = df[price_cols].diff().dropna()\n",
    "    print(\"Training VAE on return differences...\")\n",
    "    vae, enc, dec = build_vae(diffs.shape[1], STUDENT_DF)\n",
    "    es = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True)\n",
    "    rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4)\n",
    "    vae.fit(\n",
    "        diffs.values,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        validation_split=0.1,\n",
    "        callbacks=[es, rlr],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # sample latent from Student-t prior in batches to save memory\n",
    "    print(\"Sampling from Student-t prior and decoding in batches...\")\n",
    "    total = N_SCENARIOS * len(diffs)\n",
    "    rng = np.random.default_rng(42)\n",
    "    sims_chunks = []\n",
    "    CHUNK_SIZE = 1_000_000  # adjust based on available RAM\n",
    "\n",
    "    for start in range(0, total, CHUNK_SIZE):\n",
    "        end = min(start + CHUNK_SIZE, total)\n",
    "        batch_len = end - start\n",
    "\n",
    "        chi2 = rng.chisquare(STUDENT_DF, size=batch_len).astype(np.float32)\n",
    "        z_batch = (\n",
    "            rng.standard_normal((batch_len, LATENT_DIM), dtype=np.float32)\n",
    "            / np.sqrt(chi2 / STUDENT_DF)[:, None]\n",
    "        )\n",
    "\n",
    "        flat_batch = dec.predict(z_batch, batch_size=1024)\n",
    "        sims_chunks.append(flat_batch.astype(np.float32))\n",
    "\n",
    "    flat_all = np.vstack(sims_chunks)\n",
    "    sims = flat_all.reshape(N_SCENARIOS, len(diffs), diffs.shape[1])\n",
    "\n",
    "    # scale marginals\n",
    "    std_sim = sims.std(axis=(0,1))\n",
    "    std_hist = diffs.std(axis=0).values\n",
    "    sims *= (std_hist / std_sim)\n",
    "\n",
    "    last_prices = df[price_cols].iloc[-1].values\n",
    "    price_paths = last_prices + np.cumsum(sims, axis=1)\n",
    "    idx_dec = price_cols.index('financial_settlement_off_taker_pays_vth')\n",
    "    losses = (PPA_VOLUME * (PPA_STRIKE - price_paths[:, :, idx_dec])).ravel()\n",
    "\n",
    "    # cleanup\n",
    "    del sims, flat_all; tf.keras.backend.clear_session(); gc.collect()\n",
    "    return losses\n",
    "\n",
    "# -------------------- MAIN PIPELINE --------------------\n",
    "def main():\n",
    "    df, dec_col, price_cols = load_and_clean(INPUT_CSV)\n",
    "    print(\"Generating historical metrics...\")\n",
    "    hist_losses = df[dec_col].values\n",
    "\n",
    "    # Copula + EVT\n",
    "    cop_losses = run_copula(df, price_cols)\n",
    "    cop_evt = evt_boost(cop_losses)\n",
    "\n",
    "    # VAE + EVT\n",
    "    vae_losses = run_vae(df, price_cols)\n",
    "    vae_evt = evt_boost(vae_losses)\n",
    "\n",
    "    # scale final losses to historical vol\n",
    "    print(\"Final scaling to historical volatility...\")\n",
    "    hist_vol = np.std(hist_losses)\n",
    "    cop_evt *= (hist_vol / np.std(cop_evt))\n",
    "    vae_evt *= (hist_vol / np.std(vae_evt))\n",
    "\n",
    "    # assemble results\n",
    "    print(\"Computing diagnostics...\")\n",
    "    results = pd.DataFrame({\n",
    "        'Historical': compute_metrics(hist_losses),\n",
    "        't-Copula+EVT': compute_metrics(cop_evt),\n",
    "        'StudentT-VAE+EVT': compute_metrics(vae_evt)\n",
    "    }).T\n",
    "    results.to_csv(OUTPUT_CSV)\n",
    "    print(results)\n",
    "    print(f\"Results saved to {OUTPUT_CSV}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
