{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow\n",
    "!pip install keras\n",
    "!pip install optuna\n",
    "!pip install tqdm\n",
    "!pip install scipy\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install scikit-learn\n",
    "!pip install scikit-image\n",
    "!pip install scikit-learn-intelex\n",
    "!pip install cartopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from data/vypocet adapted.csv...\n",
      "Flattening headers and parsing dates...\n",
      "Converting columns to numeric and cleaning NaNs...\n",
      "Data cleaned: 8388 rows remain.\n",
      "Generating historical metrics...\n",
      "Starting t-Copula simulation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copula u-samples: 100%|██████████| 5000/5000 [00:45<00:00, 109.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverting copula samples back to returns via KDE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invert KDE per asset: 100%|██████████| 3/3 [00:08<00:00,  2.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling copula marginals to match historical volatility...\n",
      "Applying EVT tail-boost...\n",
      "EVT fit: shape=-0.24, scale=738.61\n",
      "Training VAE on return differences...\n",
      "Building Student-T VAE...\n",
      "Epoch 1/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 5.1362 - val_loss: 4.2673 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 3.6535 - val_loss: 3.4360 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.2291 - val_loss: 3.3325 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.2796 - val_loss: 3.1522 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.8130 - val_loss: 3.1250 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.6930 - val_loss: 2.9386 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.5835 - val_loss: 2.8512 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.6220 - val_loss: 2.8996 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.5301 - val_loss: 2.8538 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.4630 - val_loss: 2.7308 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.4111 - val_loss: 2.6967 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.3479 - val_loss: 2.6974 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.3904 - val_loss: 2.5957 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.2577 - val_loss: 2.6260 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.3287 - val_loss: 2.5937 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.2970 - val_loss: 2.6271 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.2404 - val_loss: 2.5741 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.1835 - val_loss: 2.5558 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.1914 - val_loss: 2.4793 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.2388 - val_loss: 2.6354 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.1464 - val_loss: 2.4385 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.1489 - val_loss: 2.4689 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.1274 - val_loss: 2.5623 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.0958 - val_loss: 2.4177 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.1155 - val_loss: 2.5445 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.1678 - val_loss: 2.4943 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.0769 - val_loss: 2.5051 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.0769 - val_loss: 2.4841 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.1671 - val_loss: 2.4640 - learning_rate: 5.0000e-04\n",
      "Epoch 30/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.0567 - val_loss: 2.4351 - learning_rate: 5.0000e-04\n",
      "Epoch 31/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.0847 - val_loss: 2.4035 - learning_rate: 5.0000e-04\n",
      "Epoch 32/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.0578 - val_loss: 2.5006 - learning_rate: 5.0000e-04\n",
      "Epoch 33/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.0225 - val_loss: 2.4472 - learning_rate: 5.0000e-04\n",
      "Epoch 34/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.0927 - val_loss: 2.4327 - learning_rate: 5.0000e-04\n",
      "Epoch 35/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.1028 - val_loss: 2.4131 - learning_rate: 5.0000e-04\n",
      "Epoch 36/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.0399 - val_loss: 2.3674 - learning_rate: 2.5000e-04\n",
      "Epoch 37/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.0408 - val_loss: 2.3670 - learning_rate: 2.5000e-04\n",
      "Epoch 38/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.0371 - val_loss: 2.3707 - learning_rate: 2.5000e-04\n",
      "Epoch 39/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.0639 - val_loss: 2.4128 - learning_rate: 2.5000e-04\n",
      "Epoch 40/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.9949 - val_loss: 2.3267 - learning_rate: 2.5000e-04\n",
      "Epoch 41/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.0470 - val_loss: 2.4246 - learning_rate: 2.5000e-04\n",
      "Epoch 42/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.0626 - val_loss: 2.3450 - learning_rate: 2.5000e-04\n",
      "Epoch 43/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.0115 - val_loss: 2.4661 - learning_rate: 2.5000e-04\n",
      "Epoch 44/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.0207 - val_loss: 2.3924 - learning_rate: 2.5000e-04\n",
      "Epoch 45/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.0021 - val_loss: 2.3749 - learning_rate: 1.2500e-04\n",
      "Epoch 46/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.0240 - val_loss: 2.3935 - learning_rate: 1.2500e-04\n",
      "Epoch 47/100\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.0204 - val_loss: 2.3977 - learning_rate: 1.2500e-04\n",
      "Sampling from Student-t prior and decoding in batches...\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step  \n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m914/914\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "Applying EVT tail-boost...\n",
      "EVT fit: shape=0.17, scale=51.85\n",
      "Final scaling to historical volatility...\n",
      "Computing diagnostics...\n",
      "                  Volatility  Skewness   Kurtosis\n",
      "Historical          4.302873  4.342788  33.331716\n",
      "t-Copula+EVT        4.302873  3.275812  18.506739\n",
      "StudentT-VAE+EVT    4.302873  0.613225   5.193272\n",
      "Results saved to output/ml_benchmark_risk_table.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "from scipy.stats import skew, kurtosis, t as student_t, genpareto, energy_distance\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import warnings\n",
    "\n",
    "# Suppress TensorFlow warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='tensorflow')\n",
    "\n",
    "# -------------------- CONFIGURATION --------------------\n",
    "INPUT_CSV    = os.getenv('INPUT_CSV', 'data/vypocet adapted.csv')\n",
    "OUTPUT_CSV   = os.getenv('OUTPUT_CSV', 'output/ml_benchmark_risk_table.csv')\n",
    "\n",
    "# scenario counts\n",
    "N_SCENARIOS  = 5000\n",
    "\n",
    "# VAE / Copula parameters\n",
    "BATCH_SIZE   = 256\n",
    "EPOCHS       = 100\n",
    "LATENT_DIM   = 12\n",
    "STUDENT_DF   = 3       # heavy-tailed latent prior for VAE\n",
    "EWMA_LAMBDA  = 0.94\n",
    "KDE_BW       = 0.1\n",
    "TAIL_QUANT   = 0.99\n",
    "\n",
    "# PPA constants\n",
    "PPA_VOLUME   = 1.0\n",
    "PPA_STRIKE   = 50.0\n",
    "\n",
    "# -------------------- DATA UTILITIES --------------------\n",
    "def load_and_clean(path):\n",
    "    print(f\"Loading data from {path}...\")\n",
    "    df0 = pd.read_csv(path, sep=';', header=[0,1], engine='python')\n",
    "    print(\"Flattening headers and parsing dates...\")\n",
    "    cols = df0.columns.tolist()\n",
    "    zones, flat = [], []\n",
    "    cur = None\n",
    "    for lvl0, lvl1 in cols:\n",
    "        if not str(lvl0).startswith('Unnamed'):\n",
    "            cur = lvl0.replace('-', '_').lower()\n",
    "        zones.append(None if lvl1.startswith('Unnamed') or lvl1=='Time period' else cur)\n",
    "    for (lvl0, lvl1), z in zip(cols, zones):\n",
    "        name = lvl1.replace(' ','_').replace('(','').replace(')','').replace('-','_').lower()\n",
    "        flat.append(f\"{name}_{z}\" if z else name)\n",
    "    df0.columns = flat\n",
    "    df0['time_period'] = df0['time_period'].str.replace(r'\\s*\\(.*\\)', '', regex=True)\n",
    "    df0['timestamp'] = pd.to_datetime(\n",
    "        df0['time_period'].str.split(' - ').str[0], dayfirst=True, errors='coerce')\n",
    "    df = df0.set_index('timestamp')\n",
    "    # select columns\n",
    "    dec_col = 'decoupling_lost_off_taker_vth'\n",
    "    price_cols = [c for c in flat if 'financial_settlement' in c]\n",
    "    # convert to numeric\n",
    "    print(\"Converting columns to numeric and cleaning NaNs...\")\n",
    "    df[[dec_col] + price_cols] = df[[dec_col] + price_cols].apply(\n",
    "        lambda s: pd.to_numeric(\n",
    "            s.astype(str).str.replace('.', '', regex=False).str.replace(',', '.', regex=False),\n",
    "            errors='coerce'\n",
    "        )\n",
    "    )\n",
    "    df.dropna(subset=[dec_col] + price_cols, inplace=True)\n",
    "    df = df[(df[price_cols] > 0).all(axis=1)]\n",
    "    print(f\"Data cleaned: {len(df)} rows remain.\")\n",
    "    return df, dec_col, price_cols\n",
    "\n",
    "# -------------------- METRICS --------------------\n",
    "def compute_metrics(losses):\n",
    "    return {\n",
    "        'Volatility': float(np.std(losses, ddof=0)),\n",
    "        'Skewness':   float(skew(losses)),\n",
    "        'Kurtosis':   float(kurtosis(losses, fisher=False))\n",
    "    }\n",
    "\n",
    "# -------------------- COPULA SIMULATION --------------------\n",
    "def simulate_t_copula(returns, n_scen, df, ewma_lambda):\n",
    "    print(\"Starting t-Copula simulation...\")\n",
    "    u = returns.rank(axis=0, method='average') / (len(returns) + 1)\n",
    "    z = student_t.ppf(u, df)\n",
    "    weights = ewma_lambda ** np.arange(len(z)-1, -1, -1)\n",
    "    weights /= weights.sum()\n",
    "    cov = np.cov(z.T, aweights=weights)\n",
    "    rng = np.random.default_rng(123)\n",
    "    chi2 = rng.chisquare(df, size=n_scen)\n",
    "    L = np.linalg.cholesky(cov)\n",
    "    sims_u = np.empty((n_scen, len(returns), returns.shape[1]))\n",
    "    for i in tqdm(range(n_scen), desc=\"Copula u-samples\"):\n",
    "        z0 = rng.standard_normal(z.shape)\n",
    "        t_val = (z0 @ L.T) / np.sqrt(chi2[i] / df)\n",
    "        sims_u[i] = student_t.cdf(t_val, df)\n",
    "    return sims_u\n",
    "\n",
    "# -------------------- KDE INVERSION --------------------\n",
    "def invert_kde(u_samples, hist_data, bandwidth):\n",
    "    vals = np.sort(hist_data)\n",
    "    kde = KernelDensity(bandwidth=bandwidth).fit(vals[:, None])\n",
    "    grid = np.linspace(vals.min(), vals.max(), 2000)[:, None]\n",
    "    logp = kde.score_samples(grid)\n",
    "    cdf = np.exp(logp).cumsum()\n",
    "    cdf /= cdf[-1]\n",
    "    inv = np.interp(u_samples.ravel(), cdf, grid.ravel())\n",
    "    return inv.reshape(u_samples.shape)\n",
    "\n",
    "# -------------------- RUN COPULA PIPELINE --------------------\n",
    "def run_copula(df, price_cols):\n",
    "    diffs = df[price_cols].diff().dropna()\n",
    "    sims_u = simulate_t_copula(diffs, N_SCENARIOS, STUDENT_DF, EWMA_LAMBDA)\n",
    "    sims = np.zeros_like(sims_u)\n",
    "    print(\"Inverting copula samples back to returns via KDE...\")\n",
    "    for j in tqdm(range(len(price_cols)), desc=\"Invert KDE per asset\"):\n",
    "        sims[:, :, j] = invert_kde(sims_u[:, :, j], diffs.iloc[:, j].values, KDE_BW)\n",
    "    print(\"Scaling copula marginals to match historical volatility...\")\n",
    "    std_sim = sims.std(axis=(0,1))\n",
    "    std_hist = diffs.std(axis=0).values\n",
    "    sims *= (std_hist / std_sim)\n",
    "    last_prices = df[price_cols].iloc[-1].values\n",
    "    price_paths = last_prices + np.cumsum(sims, axis=1)\n",
    "    idx_dec = price_cols.index('financial_settlement_off_taker_pays_vth')\n",
    "    losses = (PPA_VOLUME * (PPA_STRIKE - price_paths[:, :, idx_dec])).ravel()\n",
    "    del sims_u, sims; gc.collect()\n",
    "    return losses\n",
    "\n",
    "# -------------------- EVT BOOST --------------------\n",
    "def evt_boost(losses, quantile=TAIL_QUANT):\n",
    "    print(\"Applying EVT tail-boost...\")\n",
    "    threshold = np.quantile(losses, quantile)\n",
    "    excess = losses[losses > threshold] - threshold\n",
    "    if excess.size > 0:\n",
    "        shp, loc, scl = genpareto.fit(excess, floc=0)\n",
    "        boosted = threshold + genpareto.rvs(shp, loc=0, scale=scl, size=excess.size)\n",
    "        print(f\"EVT fit: shape={shp:.2f}, scale={scl:.2f}\")\n",
    "        return np.hstack([losses, boosted])\n",
    "    print(\"No extreme exceedances found; skipping EVT.\")\n",
    "    return losses\n",
    "\n",
    "# -------------------- VAE MODEL --------------------\n",
    "class StudentTVAE(Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def call(self, x):\n",
    "        m, lv, z = self.encoder(x)\n",
    "        recon = self.decoder(z)\n",
    "        recon_loss = tf.reduce_mean(tf.square(x - recon))\n",
    "        kl_loss = 0.5 * tf.reduce_sum(tf.exp(lv) + m**2 - 1 - lv, axis=1)\n",
    "        self.add_loss(recon_loss + tf.reduce_mean(kl_loss))\n",
    "        return recon\n",
    "\n",
    "def build_vae(n_assets, prior_df):\n",
    "    print(\"Building Student-T VAE...\")\n",
    "    inp = Input((n_assets,))\n",
    "    x = layers.Dense(128, activation='relu')(inp)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    m = layers.Dense(LATENT_DIM)(x)\n",
    "    lv = layers.Dense(LATENT_DIM)(x)\n",
    "    def sample_latent(args):\n",
    "        mu, logvar = args\n",
    "        eps = tf.random.normal(tf.shape(mu))\n",
    "        return mu + tf.exp(0.5 * logvar) * eps\n",
    "    z = layers.Lambda(sample_latent)([m, lv])\n",
    "    encoder = Model(inp, [m, lv, z], name='encoder')\n",
    "\n",
    "    latent_in = Input((LATENT_DIM,))\n",
    "    y = layers.Dense(64, activation='relu')(latent_in)\n",
    "    y = layers.BatchNormalization()(y)\n",
    "    y = layers.Dense(128, activation='relu')(y)\n",
    "    out = layers.Dense(n_assets)(y)\n",
    "    decoder = Model(latent_in, out, name='decoder')\n",
    "\n",
    "    vae = StudentTVAE(encoder, decoder)\n",
    "    vae.compile(optimizer='adam')\n",
    "    return vae, encoder, decoder\n",
    "\n",
    "# -------------------- RUN VAE PIPELINE --------------------\n",
    "def run_vae(df, price_cols):\n",
    "    diffs = df[price_cols].diff().dropna()\n",
    "    print(\"Training VAE on return differences...\")\n",
    "    vae, enc, dec = build_vae(diffs.shape[1], STUDENT_DF)\n",
    "    es = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True)\n",
    "    rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4)\n",
    "    vae.fit(\n",
    "        diffs.values,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        validation_split=0.1,\n",
    "        callbacks=[es, rlr],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # sample latent from Student-t prior in batches to save memory\n",
    "    print(\"Sampling from Student-t prior and decoding in batches...\")\n",
    "    total = N_SCENARIOS * len(diffs)\n",
    "    rng = np.random.default_rng(42)\n",
    "    sims_chunks = []\n",
    "    CHUNK_SIZE = 1_000_000  # adjust based on available RAM\n",
    "\n",
    "    for start in range(0, total, CHUNK_SIZE):\n",
    "        end = min(start + CHUNK_SIZE, total)\n",
    "        batch_len = end - start\n",
    "\n",
    "        chi2 = rng.chisquare(STUDENT_DF, size=batch_len).astype(np.float32)\n",
    "        z_batch = (\n",
    "            rng.standard_normal((batch_len, LATENT_DIM), dtype=np.float32)\n",
    "            / np.sqrt(chi2 / STUDENT_DF)[:, None]\n",
    "        )\n",
    "\n",
    "        flat_batch = dec.predict(z_batch, batch_size=1024)\n",
    "        sims_chunks.append(flat_batch.astype(np.float32))\n",
    "\n",
    "    flat_all = np.vstack(sims_chunks)\n",
    "    sims = flat_all.reshape(N_SCENARIOS, len(diffs), diffs.shape[1])\n",
    "\n",
    "    # scale marginals\n",
    "    std_sim = sims.std(axis=(0,1))\n",
    "    std_hist = diffs.std(axis=0).values\n",
    "    sims *= (std_hist / std_sim)\n",
    "\n",
    "    last_prices = df[price_cols].iloc[-1].values\n",
    "    price_paths = last_prices + np.cumsum(sims, axis=1)\n",
    "    idx_dec = price_cols.index('financial_settlement_off_taker_pays_vth')\n",
    "    losses = (PPA_VOLUME * (PPA_STRIKE - price_paths[:, :, idx_dec])).ravel()\n",
    "\n",
    "    # cleanup\n",
    "    del sims, flat_all; tf.keras.backend.clear_session(); gc.collect()\n",
    "    return losses\n",
    "\n",
    "# -------------------- MAIN PIPELINE --------------------\n",
    "def main():\n",
    "    df, dec_col, price_cols = load_and_clean(INPUT_CSV)\n",
    "    print(\"Generating historical metrics...\")\n",
    "    hist_losses = df[dec_col].values\n",
    "\n",
    "    # Copula + EVT\n",
    "    cop_losses = run_copula(df, price_cols)\n",
    "    cop_evt = evt_boost(cop_losses)\n",
    "\n",
    "    # VAE + EVT\n",
    "    vae_losses = run_vae(df, price_cols)\n",
    "    vae_evt = evt_boost(vae_losses)\n",
    "\n",
    "    # scale final losses to historical vol\n",
    "    print(\"Final scaling to historical volatility...\")\n",
    "    hist_vol = np.std(hist_losses)\n",
    "    cop_evt *= (hist_vol / np.std(cop_evt))\n",
    "    vae_evt *= (hist_vol / np.std(vae_evt))\n",
    "\n",
    "    # assemble results\n",
    "    print(\"Computing diagnostics...\")\n",
    "    results = pd.DataFrame({\n",
    "        'Historical': compute_metrics(hist_losses),\n",
    "        't-Copula+EVT': compute_metrics(cop_evt),\n",
    "        'StudentT-VAE+EVT': compute_metrics(vae_evt)\n",
    "    }).T\n",
    "    # if output directory does not exist, create it\n",
    "    os.makedirs(os.path.dirname(OUTPUT_CSV), exist_ok=True)\n",
    "    results.to_csv(OUTPUT_CSV)\n",
    "    print(results)\n",
    "    print(f\"Results saved to {OUTPUT_CSV}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
