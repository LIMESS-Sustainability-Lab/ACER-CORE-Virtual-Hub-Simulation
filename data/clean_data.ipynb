{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from os import path\n",
    "from matplotlib import pyplot, colors\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import os\n",
    "import plotly.express as px\n",
    "# from tobler.area_weighted import area_interpolate\n",
    "import json\n",
    "from shapely.geometry import Point, shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps to check for anomalies and clean:\n",
    "\n",
    "1. Find missing values\n",
    "\n",
    "2. Check for duplicates\n",
    "\n",
    "3. Check for leading and trailing whitespaces\n",
    "\n",
    "4. Check for invalid dates (like the ones where it may be interpreted as 1900-01-01)\n",
    "\n",
    "5. Save as a new clean CSV to be used for further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate rows: 0\n",
      "Duplicate rows: 0\n",
      "Duplicate rows: 0\n",
      "Duplicate rows: 0\n",
      "Duplicate rows: 0\n",
      "Processed and saved: processed/spot_prices_cleaned\\AT_processed.csv\n",
      "Duplicate rows: 0\n",
      "Duplicate rows: 0\n",
      "Duplicate rows: 0\n",
      "Duplicate rows: 0\n",
      "Duplicate rows: 0\n",
      "Processed and saved: processed/spot_prices_cleaned\\BE_processed.csv\n",
      "Duplicate rows: 0\n",
      "Duplicate rows: 0\n",
      "Duplicate rows: 0\n",
      "Duplicate rows: 0\n",
      "Duplicate rows: 0\n",
      "Processed and saved: processed/spot_prices_cleaned\\CZ_processed.csv\n",
      "Duplicate rows: 0\n",
      "Duplicate rows: 0\n",
      "Duplicate rows: 0\n",
      "Duplicate rows: 0\n",
      "Duplicate rows: 0\n",
      "Processed and saved: processed/spot_prices_cleaned\\DE_LU_processed.csv\n",
      "Duplicate rows: 0\n",
      "Duplicate rows: 0\n",
      "Duplicate rows: 0\n",
      "Duplicate rows: 0\n",
      "Duplicate rows: 0\n",
      "Processed and saved: processed/spot_prices_cleaned\\FR_processed.csv\n",
      "Duplicate rows: 0\n",
      "Duplicate rows: 0\n",
      "Duplicate rows: 0\n",
      "Duplicate rows: 0\n",
      "Duplicate rows: 0\n",
      "Processed and saved: processed/spot_prices_cleaned\\HR_processed.csv\n",
      "Duplicate rows: 0\n",
      "Duplicate rows: 0\n",
      "Duplicate rows: 0\n",
      "Duplicate rows: 0\n",
      "Duplicate rows: 0\n",
      "Processed and saved: processed/spot_prices_cleaned\\HU_processed.csv\n",
      "Duplicate rows: 0\n",
      "Duplicate rows: 0\n",
      "Duplicate rows: 0\n",
      "Duplicate rows: 0\n",
      "Duplicate rows: 0\n",
      "Processed and saved: processed/spot_prices_cleaned\\NL_processed.csv\n",
      "Duplicate rows: 0\n",
      "Duplicate rows: 0\n",
      "Duplicate rows: 0\n",
      "Duplicate rows: 0\n",
      "Duplicate rows: 0\n",
      "Processed and saved: processed/spot_prices_cleaned\\PL_processed.csv\n",
      "Duplicate rows: 0\n",
      "Duplicate rows: 0\n",
      "Duplicate rows: 0\n",
      "Duplicate rows: 0\n",
      "Duplicate rows: 0\n",
      "Processed and saved: processed/spot_prices_cleaned\\RO_processed.csv\n",
      "Duplicate rows: 0\n",
      "Duplicate rows: 0\n",
      "Duplicate rows: 0\n",
      "Duplicate rows: 0\n",
      "Duplicate rows: 0\n",
      "Processed and saved: processed/spot_prices_cleaned\\SI_processed.csv\n",
      "Duplicate rows: 0\n",
      "Duplicate rows: 0\n",
      "Duplicate rows: 0\n",
      "Duplicate rows: 0\n",
      "Duplicate rows: 0\n",
      "Processed and saved: processed/spot_prices_cleaned\\SK_processed.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the folder containing the CSV files\n",
    "folder_path = \"raw/spot_prices\"\n",
    "\n",
    "# Define a function to clean a single DataFrame\n",
    "def clean_data(df):\n",
    "    # # Step 1: Find and handle missing values\n",
    "    # missing_values = df.isnull().sum()\n",
    "    # print(\"Missing values:\\n\", missing_values)\n",
    "    # df = df.dropna()  # Drop rows with missing values (or use df.fillna() to fill them)\n",
    "\n",
    "    # Step 2: Check for duplicates\n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(\"Duplicate rows:\", duplicates)\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    # Step 3: Check for leading and trailing whitespaces\n",
    "    for col in df.select_dtypes(include=[\"object\"]).columns:\n",
    "        df[col] = df[col].str.strip()\n",
    "\n",
    "    # Step 4: Clean and parse the MTU (CET/CEST) column\n",
    "    # Remove extraneous text like \"(CET)\" or \"(CEST)\"\n",
    "    df[\"MTU (CET/CEST)\"] = df[\"MTU (CET/CEST)\"].str.replace(\"(CET)\", \"\", regex=False).str.replace(\"(CEST)\", \"\", regex=False).str.strip()\n",
    "\n",
    "    # Step 4: Check for invalid dates\n",
    "    # df[\"Start Time\"] = pd.to_datetime(df[\"MTU (CET/CEST)\"].str.split(\" - \").str[0], errors=\"coerce\")\n",
    "    # df[\"End Time\"] = pd.to_datetime(df[\"MTU (CET/CEST)\"].str.split(\" - \").str[1], errors=\"coerce\")\n",
    "    # invalid_dates = df[df[\"Start Time\"].isnull() | df[\"End Time\"].isnull()]\n",
    "    # print(\"Invalid dates:\\n\", invalid_dates)\n",
    "    # df = df.dropna(subset=[\"Start Time\", \"End Time\"])  # Drop rows with invalid dates\n",
    "\n",
    "    return df\n",
    "\n",
    "# Iterate through the subfolders (countries) in the folder\n",
    "output_folder = \"processed/spot_prices_cleaned\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "for country_folder in os.listdir(folder_path):\n",
    "    country_path = os.path.join(folder_path, country_folder)\n",
    "    if os.path.isdir(country_path):\n",
    "        # Initialize an empty DataFrame to store combined data for the country\n",
    "        combined_df = pd.DataFrame()\n",
    "        # Iterate through the CSV files in the country's folder\n",
    "        for file_name in os.listdir(country_path):\n",
    "            if file_name.endswith(\".csv\"):\n",
    "                file_path = os.path.join(country_path, file_name)\n",
    "                # Read the CSV file\n",
    "                df = pd.read_csv(file_path)\n",
    "                # Clean the data\n",
    "                cleaned_df = clean_data(df)\n",
    "                # Append the cleaned data to the combined DataFrame\n",
    "                combined_df = pd.concat([combined_df, cleaned_df], ignore_index=True)\n",
    "\n",
    "        # Save the combined data for the country to a single CSV file\n",
    "        output_file = os.path.join(output_folder, f\"{country_folder}_processed.csv\")\n",
    "        combined_df.to_csv(output_file, index=False)\n",
    "        print(f\"Processed and saved: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting file: AT_processed.csv, Total rows: 350592\n",
      "Saved formatted file: processed/spot_prices_quarter_hours\\AT_processed.csv\n",
      "Formatting file: BE_processed.csv, Total rows: 43824\n",
      "Saved formatted file: processed/spot_prices_quarter_hours\\BE_processed.csv\n",
      "Formatting file: CZ_processed.csv, Total rows: 43824\n",
      "Saved formatted file: processed/spot_prices_quarter_hours\\CZ_processed.csv\n",
      "Formatting file: DE_LU_processed.csv, Total rows: 350592\n",
      "Saved formatted file: processed/spot_prices_quarter_hours\\DE_LU_processed.csv\n",
      "Formatting file: FR_processed.csv, Total rows: 43824\n",
      "Saved formatted file: processed/spot_prices_quarter_hours\\FR_processed.csv\n",
      "Formatting file: HR_processed.csv, Total rows: 43824\n",
      "Saved formatted file: processed/spot_prices_quarter_hours\\HR_processed.csv\n",
      "Formatting file: HU_processed.csv, Total rows: 43824\n",
      "Saved formatted file: processed/spot_prices_quarter_hours\\HU_processed.csv\n",
      "Formatting file: NL_processed.csv, Total rows: 43824\n",
      "Saved formatted file: processed/spot_prices_quarter_hours\\NL_processed.csv\n",
      "Formatting file: PL_processed.csv, Total rows: 43824\n",
      "Saved formatted file: processed/spot_prices_quarter_hours\\PL_processed.csv\n",
      "Formatting file: RO_processed.csv, Total rows: 43824\n",
      "Saved formatted file: processed/spot_prices_quarter_hours\\RO_processed.csv\n",
      "Formatting file: SI_processed.csv, Total rows: 43824\n",
      "Saved formatted file: processed/spot_prices_quarter_hours\\SI_processed.csv\n",
      "Formatting file: SK_processed.csv, Total rows: 43824\n",
      "Saved formatted file: processed/spot_prices_quarter_hours\\SK_processed.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the folder containing the processed CSV files\n",
    "input_folder = \"processed/spot_prices_cleaned\"\n",
    "output_folder = \"processed/spot_prices_quarter_hours\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Function to expand rows into quarter-hour intervals\n",
    "def format_to_quarter_hours(df):\n",
    "    quarter_hour_rows = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        try:\n",
    "            # Parse the start and end times\n",
    "            start_time = pd.to_datetime(row[\"MTU (CET/CEST)\"].split(\" - \")[0], dayfirst=True)\n",
    "            end_time = pd.to_datetime(row[\"MTU (CET/CEST)\"].split(\" - \")[1], dayfirst=True)\n",
    "\n",
    "            # Generate quarter-hour intervals\n",
    "            current_time = start_time\n",
    "            while current_time < end_time:\n",
    "                next_time = current_time + pd.Timedelta(minutes=15)\n",
    "                quarter_hour_rows.append({\n",
    "                    \"MTU (CET/CEST)\": f\"{current_time.strftime('%d/%m/%Y %H:%M:%S')} - {next_time.strftime('%d/%m/%Y %H:%M:%S')}\",\n",
    "                    \"Area\": row[\"Area\"],\n",
    "                    \"Sequence\": row[\"Sequence\"],\n",
    "                    \"Day-ahead Price (EUR/MWh)\": row[\"Day-ahead Price (EUR/MWh)\"],\n",
    "                    \"Intraday Period (CET/CEST)\": row[\"Intraday Period (CET/CEST)\"],\n",
    "                    \"Intraday Price (EUR/MWh)\": row[\"Intraday Price (EUR/MWh)\"]\n",
    "                })\n",
    "                # print(f\"Processing row: {row['MTU (CET/CEST)']}, Current Time: {current_time}\")\n",
    "                current_time = next_time\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row {index}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Create a new DataFrame with quarter-hour intervals\n",
    "    return pd.DataFrame(quarter_hour_rows)\n",
    "\n",
    "# Iterate through the processed files\n",
    "for file_name in os.listdir(input_folder):\n",
    "    if file_name.endswith(\".csv\"):\n",
    "        file_path = os.path.join(input_folder, file_name)\n",
    "        # Read the processed CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Formatting file: {file_name}, Total rows: {len(df)}\")\n",
    "        # Format the data into quarter-hour intervals\n",
    "        formatted_df = format_to_quarter_hours(df)\n",
    "        # Save the formatted data to a new CSV file\n",
    "        output_file = os.path.join(output_folder, file_name)\n",
    "        formatted_df.to_csv(output_file, index=False)\n",
    "        print(f\"Saved formatted file: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved: processed/load_combined\\AT_load_combined.csv\n",
      "Processed and saved: processed/load_combined\\BE_load_combined.csv\n",
      "Processed and saved: processed/load_combined\\CZ_load_combined.csv\n",
      "Processed and saved: processed/load_combined\\DE_LU_load_combined.csv\n",
      "Processed and saved: processed/load_combined\\FR_load_combined.csv\n",
      "Processed and saved: processed/load_combined\\HR_load_combined.csv\n",
      "Processed and saved: processed/load_combined\\HU_load_combined.csv\n",
      "Processed and saved: processed/load_combined\\NL_load_combined.csv\n",
      "Processed and saved: processed/load_combined\\PL_load_combined.csv\n",
      "Processed and saved: processed/load_combined\\RO_load_combined.csv\n",
      "Processed and saved: processed/load_combined\\SI_load_combined.csv\n",
      "Processed and saved: processed/load_combined\\SK_load_combined.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the folder containing the raw load files\n",
    "input_folder = \"raw/load\"\n",
    "output_folder = \"processed/load_combined\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Iterate through the subfolders (countries) in the folder\n",
    "for country_folder in os.listdir(input_folder):\n",
    "    country_path = os.path.join(input_folder, country_folder)\n",
    "    if os.path.isdir(country_path):\n",
    "        # Initialize an empty DataFrame to store combined data for the country\n",
    "        combined_df = pd.DataFrame()\n",
    "\n",
    "        # Iterate through the CSV files in the country's folder\n",
    "        for file_name in os.listdir(country_path):\n",
    "            if file_name.endswith(\".csv\"):\n",
    "                file_path = os.path.join(country_path, file_name)\n",
    "                # Read the CSV file\n",
    "                df = pd.read_csv(file_path)\n",
    "                # Append the data to the combined DataFrame\n",
    "                combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "\n",
    "        # Save the combined data for the country to a single CSV file\n",
    "        output_file = os.path.join(output_folder, f\"{country_folder}_load_combined.csv\")\n",
    "        combined_df.to_csv(output_file, index=False)\n",
    "        print(f\"Processed and saved: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated and saved: processed/load_aggregated\\AT_load_aggregated.csv\n",
      "Aggregated and saved: processed/load_aggregated\\BE_load_aggregated.csv\n",
      "Aggregated and saved: processed/load_aggregated\\CZ_load_aggregated.csv\n",
      "Aggregated and saved: processed/load_aggregated\\DE_LU_load_aggregated.csv\n",
      "Aggregated and saved: processed/load_aggregated\\FR_load_aggregated.csv\n",
      "Aggregated and saved: processed/load_aggregated\\HR_load_aggregated.csv\n",
      "Aggregated and saved: processed/load_aggregated\\HU_load_aggregated.csv\n",
      "Aggregated and saved: processed/load_aggregated\\NL_load_aggregated.csv\n",
      "Aggregated and saved: processed/load_aggregated\\PL_load_aggregated.csv\n",
      "Aggregated and saved: processed/load_aggregated\\RO_load_aggregated.csv\n",
      "Aggregated and saved: processed/load_aggregated\\SI_load_aggregated.csv\n",
      "Aggregated and saved: processed/load_aggregated\\SK_load_aggregated.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the folder containing the combined load files\n",
    "input_folder = \"processed/load_combined\"\n",
    "output_folder = \"processed/load_aggregated\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Iterate through the combined files for each country\n",
    "for file_name in os.listdir(input_folder):\n",
    "    if file_name.endswith(\".csv\"):\n",
    "        file_path = os.path.join(input_folder, file_name)\n",
    "        # Read the combined CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Extract the country code from the file name (e.g., \"AT\" from \"AT_load_combined.csv\")\n",
    "        if \"DE_LU\" in file_name:\n",
    "            country_code = \"DE-LU\"\n",
    "        else:\n",
    "            country_code = file_name.split(\"_\")[0]\n",
    "\n",
    "        # Dynamically set column names based on the country code\n",
    "        day_ahead_column = f\"Day-ahead Total Load Forecast [MW] - BZN|{country_code}\"\n",
    "        actual_load_column = f\"Actual Total Load [MW] - BZN|{country_code}\"\n",
    "\n",
    "        # Parse the \"Time (CET/CEST)\" column into start and end times\n",
    "        df[\"Start Time\"] = pd.to_datetime(df[\"Time (CET/CEST)\"].str.split(\" - \").str[0], format=\"%d.%m.%Y %H:%M\")\n",
    "        df[\"End Time\"] = pd.to_datetime(df[\"Time (CET/CEST)\"].str.split(\" - \").str[1], format=\"%d.%m.%Y %H:%M\")\n",
    "\n",
    "        # Aggregate the data by day (sum of 15-minute intervals per day)\n",
    "        df[\"Date\"] = df[\"Start Time\"].dt.date  # Extract the date\n",
    "        daily_sum = df.groupby(\"Date\").agg({\n",
    "            day_ahead_column: \"sum\",\n",
    "            actual_load_column: \"sum\"\n",
    "        }).reset_index()\n",
    "\n",
    "        # Save the aggregated data to a new CSV file\n",
    "        output_file = os.path.join(output_folder, f\"{file_name.replace('_load_combined.csv', '_load_aggregated.csv')}\")\n",
    "        daily_sum.to_csv(output_file, index=False)\n",
    "        print(f\"Aggregated and saved: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved: processed/generation_combined\\AT_generation_combined.csv\n",
      "Processed and saved: processed/generation_combined\\BE_generation_combined.csv\n",
      "Processed and saved: processed/generation_combined\\CZ_generation_combined.csv\n",
      "Processed and saved: processed/generation_combined\\DE_LU_generation_combined.csv\n",
      "Processed and saved: processed/generation_combined\\FR_generation_combined.csv\n",
      "Processed and saved: processed/generation_combined\\HR_generation_combined.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amic\\AppData\\Local\\Temp\\ipykernel_13672\\2160230102.py:21: DtypeWarning: Columns (20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved: processed/generation_combined\\HU_generation_combined.csv\n",
      "Processed and saved: processed/generation_combined\\NL_generation_combined.csv\n",
      "Processed and saved: processed/generation_combined\\PL_generation_combined.csv\n",
      "Processed and saved: processed/generation_combined\\RO_generation_combined.csv\n",
      "Processed and saved: processed/generation_combined\\SI_generation_combined.csv\n",
      "Processed and saved: processed/generation_combined\\SK_generation_combined.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the folder containing the raw generation files\n",
    "input_folder = \"raw/generation\"\n",
    "output_folder = \"processed/generation_combined\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Iterate through the subfolders (countries) in the folder\n",
    "for country_folder in os.listdir(input_folder):\n",
    "    country_path = os.path.join(input_folder, country_folder)\n",
    "    if os.path.isdir(country_path):\n",
    "        # Initialize an empty DataFrame to store combined data for the country\n",
    "        combined_df = pd.DataFrame()\n",
    "\n",
    "        # Iterate through the CSV files in the country's folder\n",
    "        for file_name in os.listdir(country_path):\n",
    "            if file_name.endswith(\".csv\"):\n",
    "                file_path = os.path.join(country_path, file_name)\n",
    "                # Read the CSV file\n",
    "                df = pd.read_csv(file_path)\n",
    "                # Append the data to the combined DataFrame\n",
    "                combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "\n",
    "        # Save the combined data for the country to a single CSV file\n",
    "        output_file = os.path.join(output_folder, f\"{country_folder}_generation_combined.csv\")\n",
    "        combined_df.to_csv(output_file, index=False)\n",
    "        print(f\"Processed and saved: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amic\\AppData\\Local\\Temp\\ipykernel_13672\\3378439570.py:24: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace(\"n/e\", 0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated and saved: processed/generation_aggregated\\AT_generation_aggregated.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amic\\AppData\\Local\\Temp\\ipykernel_13672\\3378439570.py:24: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace(\"n/e\", 0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated and saved: processed/generation_aggregated\\BE_generation_aggregated.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amic\\AppData\\Local\\Temp\\ipykernel_13672\\3378439570.py:24: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace(\"n/e\", 0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated and saved: processed/generation_aggregated\\CZ_generation_aggregated.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amic\\AppData\\Local\\Temp\\ipykernel_13672\\3378439570.py:14: DtypeWarning: Columns (5,17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\Amic\\AppData\\Local\\Temp\\ipykernel_13672\\3378439570.py:24: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace(\"n/e\", 0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated and saved: processed/generation_aggregated\\DE_LU_generation_aggregated.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amic\\AppData\\Local\\Temp\\ipykernel_13672\\3378439570.py:24: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace(\"n/e\", 0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated and saved: processed/generation_aggregated\\FR_generation_aggregated.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amic\\AppData\\Local\\Temp\\ipykernel_13672\\3378439570.py:24: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace(\"n/e\", 0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated and saved: processed/generation_aggregated\\HR_generation_aggregated.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amic\\AppData\\Local\\Temp\\ipykernel_13672\\3378439570.py:14: DtypeWarning: Columns (7,11,20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\Amic\\AppData\\Local\\Temp\\ipykernel_13672\\3378439570.py:24: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace(\"n/e\", 0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated and saved: processed/generation_aggregated\\HU_generation_aggregated.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amic\\AppData\\Local\\Temp\\ipykernel_13672\\3378439570.py:24: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace(\"n/e\", 0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated and saved: processed/generation_aggregated\\NL_generation_aggregated.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amic\\AppData\\Local\\Temp\\ipykernel_13672\\3378439570.py:14: DtypeWarning: Columns (20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\Amic\\AppData\\Local\\Temp\\ipykernel_13672\\3378439570.py:24: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace(\"n/e\", 0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated and saved: processed/generation_aggregated\\PL_generation_aggregated.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amic\\AppData\\Local\\Temp\\ipykernel_13672\\3378439570.py:24: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace(\"n/e\", 0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated and saved: processed/generation_aggregated\\RO_generation_aggregated.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amic\\AppData\\Local\\Temp\\ipykernel_13672\\3378439570.py:24: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace(\"n/e\", 0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated and saved: processed/generation_aggregated\\SI_generation_aggregated.csv\n",
      "Aggregated and saved: processed/generation_aggregated\\SK_generation_aggregated.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amic\\AppData\\Local\\Temp\\ipykernel_13672\\3378439570.py:24: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace(\"n/e\", 0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the folder containing the combined generation files\n",
    "input_folder = \"processed/generation_combined\"\n",
    "output_folder = \"processed/generation_aggregated\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Iterate through the combined files for each country\n",
    "for file_name in os.listdir(input_folder):\n",
    "    if file_name.endswith(\".csv\"):\n",
    "        file_path = os.path.join(input_folder, file_name)\n",
    "        # Read the combined CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Remove \"(CET/CEST)\" from the MTU column\n",
    "        df[\"MTU\"] = df[\"MTU\"].str.replace(\"(CET/CEST)\", \"\", regex=False).str.strip()\n",
    "\n",
    "        # Parse the cleaned \"MTU\" column into start and end times\n",
    "        df[\"Start Time\"] = pd.to_datetime(df[\"MTU\"].str.split(\" - \").str[0], format=\"%d.%m.%Y %H:%M\")\n",
    "        df[\"End Time\"] = pd.to_datetime(df[\"MTU\"].str.split(\" - \").str[1], format=\"%d.%m.%Y %H:%M\")\n",
    "\n",
    "        # Replace \"n/e\" with 0 for numerical aggregation\n",
    "        df.replace(\"n/e\", 0, inplace=True)\n",
    "\n",
    "        # Exclude non-numerical columns from aggregation\n",
    "        numerical_columns = df.select_dtypes(include=[\"number\"]).columns\n",
    "\n",
    "        # Aggregate the data by day (sum of 15-minute intervals per day)\n",
    "        df[\"Date\"] = df[\"Start Time\"].dt.date  # Extract the date\n",
    "        daily_sum = df.groupby(\"Date\")[numerical_columns].sum().reset_index()\n",
    "\n",
    "        # Save the aggregated data to a new CSV file\n",
    "        output_file = os.path.join(output_folder, f\"{file_name.replace('_generation_combined.csv', '_generation_aggregated.csv')}\")\n",
    "        daily_sum.to_csv(output_file, index=False)\n",
    "        print(f\"Aggregated and saved: {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
